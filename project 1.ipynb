{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "project 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    @staticmethod\n",
    "    def process(doc_query):\n",
    "       res = Preprocessor.tokenizeDocument(doc_query)\n",
    "       return res\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizeDocument(sentence):\n",
    "        return sentence.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class IndexModel:\n",
    "\n",
    "    def __init__(self, documents_df,index_file=None ,meta_file=None):\n",
    "        \n",
    "        self._index = {}\n",
    "        self.create_new_index(documents_df)\n",
    "        \n",
    "        if index_file==None and meta_file==None :\n",
    "           self.create_new_index\n",
    "           \n",
    "        else :\n",
    "            self.read_index    \n",
    "        \n",
    "\n",
    "    def create_new_index(self, documents_df):\n",
    "        \n",
    "        termdoc = documents_df.to_dict('list')\n",
    "        unique_terms = set()\n",
    "        doc_ids = termdoc['id']\n",
    "\n",
    "       \n",
    "        for terms in termdoc['ntext']:\n",
    "            unique_terms.update(terms)\n",
    "        \n",
    "        term_ids = sorted(list(unique_terms))  \n",
    "        term_to_id = {term: idx for idx, term in enumerate(term_ids)}\n",
    "        doc_to_id = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}\n",
    "        \n",
    "        \n",
    "        self._index_matrix = [[0 for _ in range(len(doc_ids))] for _ in range(len(term_ids))]\n",
    "        \n",
    "        \n",
    "        for i, terms in enumerate(termdoc['ntext']):\n",
    "            for term in terms:\n",
    "                term_idx = term_to_id[term]\n",
    "                doc_idx = doc_to_id[doc_ids[i]]\n",
    "                self._index_matrix[term_idx][doc_idx] = 1\n",
    "\n",
    "        self._term_to_id = term_to_id\n",
    "        self._doc_to_id = doc_to_id\n",
    "\n",
    "    def get_term_vector(self, term):  \n",
    "        \n",
    "        if term in self._term_to_id:\n",
    "            term_idx = self._term_to_id[term]\n",
    "            return self._index_matrix[term_idx]\n",
    "        else:\n",
    "            return [0 for _ in range(len(self._doc_to_id))]\n",
    "        \n",
    "    \n",
    "    def read_index(self, index_file, meta_file):\n",
    "        \n",
    "        \n",
    "        with open(index_file, 'rb') as f:\n",
    "            self._index_matrix = pickle.load(f)\n",
    "        \n",
    "        \n",
    "        with open(meta_file, 'rb') as f:\n",
    "            meta_data = pickle.load(f)\n",
    "        \n",
    "        self._term_to_id = meta_data['term_to_id']\n",
    "        self._doc_to_id = meta_data['doc_to_id']\n",
    "        \n",
    "        \n",
    "    def save_index(self, index_file, meta_file):\n",
    "        with open(index_file, 'wb') as f:\n",
    "            pickle.dump(self._index_matrix, f)\n",
    "\n",
    "        meta_data = {\n",
    "            'term_to_id': self._term_to_id,\n",
    "            'doc_to_id': self._doc_to_id\n",
    "        }\n",
    "\n",
    "        with open(meta_file, 'wb') as f:\n",
    "            pickle.dump(meta_data, f)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "       \n",
    "\n",
    " \n",
    "                \n",
    "        \n",
    "          \n",
    "        \n",
    "    \n",
    "        \n",
    "                        \n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Retriever:\n",
    "    def __init__(self):\n",
    "        self._terms_operator = ['&', '|', '~']\n",
    "\n",
    "    def boolean_operator_processing(self, bop, prevV, nextV=None):\n",
    "        if bop == \"&\":\n",
    "            return [a & b for a, b in zip(prevV, nextV)] # zip بحيث كل عنصر من المجموعة الاولى بشوفها مع عناصر المصفوفة التانية ونتبهي بنية معطياتو تبل \n",
    "        elif bop==\"|\" :\n",
    "            return [a | b for a, b in zip(prevV, nextV)]\n",
    "        elif bop == \"~\":\n",
    "            return [1-a for a in prevV]\n",
    "\n",
    "    def retrieve(self, query_terms, index_model):\n",
    "        ret_docs = []\n",
    "        bitwiseop=\"\"\n",
    "        result=[]\n",
    "        has_previous_term=False\n",
    "        has_not_operation=False\n",
    "        inc_vec_prev=[]\n",
    "        inc_vec_next=[]\n",
    "        for term in query_terms:\n",
    "            if term not in self._terms_operator:\n",
    "                if has_not_operation:\n",
    "                    if has_previous_term:\n",
    "                        inc_vec_next=self.boolean_operator_processing(\"~\",index_model.term_incidence_vector(term),inc_vec_next)\n",
    "                    else :\n",
    "                        inc_vec_prev=self.boolean_operator_processing(\"~\",index_model.term_incidence_vector(term),inc_vec_next)\n",
    "                        result=inc_vec_prev\n",
    "                    has_not_operation=False\n",
    "                elif has_previous_term:\n",
    "                    inc_vec_next=index_model.term_incidence_vector(term)\n",
    "                else:\n",
    "                    inc_vec_prev=index_model.term_incidence_vector(term)\n",
    "                    result= inc_vec_prev\n",
    "                    has_previous_term=True\n",
    "            elif term ==\"~\":\n",
    "                has_not_operation=True\n",
    "            else:\n",
    "                bitwiseop=term\n",
    "\n",
    "            #----------\n",
    "            if len(inc_vec_next)!= 0  :\n",
    "                result = self.boolean_operator_processing(bitwiseop,inc_vec_prev,inc_vec_next)\n",
    "                inc_vec_prev=result\n",
    "                has_previous_term=True\n",
    "                inc_vec_next= []\n",
    "\n",
    "        #-----\n",
    "        for i,res in enumerate(result):\n",
    "            if res == 1:\n",
    "                ret_docs.append({'id':i, 'score':res})\n",
    "        ret_docs = pd.DataFrame(ret_docs, columns=['id', 'score', 'content']).sort_values(by=['score'], ascending=False)\n",
    "        return ret_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text\n",
      "0  African  ﻿If you want to go fast, go alone. If you want...\n",
      "1     Bill  ﻿A successful team is a group of many hands bu...\n",
      "2    Funny  ﻿A team is like a pack of wolves—always hungry...\n",
      "3    Helen  ﻿Alone, we can do so little; together, we can ...\n",
      "4    Henry  ﻿Coming together is a beginning. Keeping toget...\n"
     ]
    }
   ],
   "source": [
    "def loadDocuments(directory):\n",
    "     documents = []\n",
    "     for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                doc_id = os.path.splitext(filename)[0]  \n",
    "                documents.append([doc_id, content])\n",
    "\n",
    "     documents_df = pd.DataFrame(documents, columns=['id', 'text'])\n",
    "     return documents_df\n",
    " \n",
    "directory = 'Project1_datacoll'\n",
    "documents_df = loadDocuments(directory)\n",
    "print(documents_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ex :\n",
    "# documents = {\n",
    "#     'id': [\"TheJungle\", \"FruitTypes\", \"HealthyLife\"],\n",
    "#     'ntext': [[\"tree\", \"garden\", \"juice\"], [\"tree\", \"apple\"], [\"apple\", \"fruit\", \"juice\"]]\n",
    "# }\n",
    "\n",
    "# documents_df = pd.DataFrame(documents)\n",
    "# index_model = IndexModel(documents_df)\n",
    "\n",
    "# print(\"TermId:\", index_model._term_to_id)\n",
    "# print(\"DocId:\", index_model._doc_to_id)\n",
    "# print(\"Index Matrix:\")\n",
    "# for row in index_model._index_matrix:\n",
    "#     print(row)\n",
    "\n",
    "# # Testing term incidence vectors\n",
    "# print(\"Incidence vector for 'tree':\", index_model.term_incidence_vector(\"tree\"))\n",
    "# print(\"Incidence vector for 'juice':\", index_model.term_incidence_vector(\"juice\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ex2 : \n",
    "\n",
    "# data = {\n",
    "#     'id': [1, 2, 3],\n",
    "#     'ntext': [['hello', 'world'], ['example', 'text'], ['hello', 'example']]\n",
    "# }\n",
    "\n",
    "\n",
    "# documents_df = pd.DataFrame(data)\n",
    "\n",
    "# index_model = IndexModel(documents_df)\n",
    "\n",
    "\n",
    "# index_model.save_index('myindex.index', 'td_ids.meta')\n",
    "\n",
    "\n",
    "# index_model.read_index('myindex.index', 'td_ids.meta')\n",
    "\n",
    "\n",
    "# term_vector = index_model.get_term_vector('hello')\n",
    "# print(term_vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine:\n",
    "\n",
    "    def __init__(self, preprocessor, retriever, documents):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.retriever = retriever\n",
    "        self.documents = None\n",
    "        self.model = None\n",
    "        self.rebuild(documents)\n",
    "\n",
    "    # offline\n",
    "    def rebuild(self, documents):\n",
    "        self.documents = documents\n",
    "        self.documents['ntext'] = self.documents['text'].apply(self.preprocessor.process)\n",
    "        self.model = IndexModel(self.documents)\n",
    "\n",
    "    # online\n",
    "    def querying(self, query):\n",
    "        query_terms = self.preprocessor.process(query)\n",
    "        docs_res = self.retriever.retrieve(query_terms, self.model)\n",
    "        if docs_res.shape[0]>0: #لعرض كلشي شغل شتغتلو\n",
    "            docs_res['content'] = docs_res.apply(\n",
    "                lambda row: self.documents[self.documents['id']==row.id]['text'].iloc[0], axis = 1)\n",
    "        return docs_res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProject1_datacoll\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m documents_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_documents\u001b[49m(directory)\n\u001b[0;32m      5\u001b[0m index_model \u001b[38;5;241m=\u001b[39m IndexModel(documents_df)\n\u001b[0;32m      8\u001b[0m search_engine \u001b[38;5;241m=\u001b[39m SearchEngine(index_model)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_documents' is not defined"
     ]
    }
   ],
   "source": [
    "directory = 'Project1_datacoll'\n",
    "documents_df = load_documents(directory)\n",
    "\n",
    "\n",
    "index_model = IndexModel(documents_df)\n",
    "\n",
    "\n",
    "search_engine = SearchEngine(index_model)\n",
    "\n",
    "\n",
    "query1 = \"success fast\"\n",
    "query2 = \"success together\"\n",
    "\n",
    "\n",
    "print(search_engine.search(query1))\n",
    "\n",
    "\n",
    "print(search_engine.search(query2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# class IndexModel:\n",
    "#     def __init__(self, documents_df=None, index_path=None, meta_path=None):\n",
    "#         self.documents_df = documents_df\n",
    "#         self.term_id = {}\n",
    "#         self.doc_id = {}\n",
    "#         self.tdim_index = []\n",
    "\n",
    "#         if index_path and meta_path:\n",
    "#             self.index_read(index_path, meta_path)\n",
    "#         elif documents_df is not None:\n",
    "#             self.index_new_create()\n",
    "\n",
    "#     def index_new_create(self):\n",
    "#         termdoc = self.documents_df.to_dict('list')\n",
    "\n",
    "#         # تحديد الأرقام التعريفية للوثائق\n",
    "#         self.doc_id = {doc: i for i, doc in enumerate(termdoc['id'])}\n",
    "\n",
    "#         # جمع المصطلحات الفريدة وتحديد الأرقام التعريفية للمصطلحات\n",
    "#         uniqterm = set()\n",
    "#         for terms in termdoc['ntext']:\n",
    "#             uniqterm.update(terms)\n",
    "#         self.term_id = {term: i for i, term in enumerate(uniqterm)}\n",
    "\n",
    "#         # إنشاء مصفوفة TDIM\n",
    "#         num_terms = len(self.term_id)\n",
    "#         num_docs = len(self.doc_id)\n",
    "#         self.tdim_index = [[0] * num_docs for _ in range(num_terms)]\n",
    "\n",
    "#         # تعبئة مصفوفة TDIM\n",
    "#         for i, doc_terms in enumerate(termdoc['ntext']):\n",
    "#             for term in doc_terms:\n",
    "#                 term_index = self.term_id[term]\n",
    "#                 doc_index = self.doc_id[termdoc['id'][i]]\n",
    "#                 self.tdim_index[term_index][doc_index] = 1\n",
    "\n",
    "#     def index_read(self, index_path, meta_path):\n",
    "#         import pickle\n",
    "        \n",
    "#         # قراءة ملف الفهرس\n",
    "#         with open(index_path, 'rb') as f:\n",
    "#             self.tdim_index = pickle.load(f)\n",
    "        \n",
    "#         # قراءة ملف معلومات الفهرس\n",
    "#         with open(meta_path, 'rb') as f:\n",
    "#             meta_data = pickle.load(f)\n",
    "#             self.term_id = meta_data[0]\n",
    "#             self.doc_id = meta_data[1]\n",
    "\n",
    "#     def vector_term_get(self, term):\n",
    "#         try:\n",
    "#             return self.tdim_index[self.term_id[term]]\n",
    "#         except KeyError:\n",
    "#             return [0] * len(self.doc_id)\n",
    "\n",
    "#     def save_index(self, index_path, meta_path):\n",
    "#         import pickle\n",
    "        \n",
    "#         # حفظ مصفوفة الفهرس\n",
    "#         with open(index_path, 'wb') as f:\n",
    "#             pickle.dump(self.tdim_index, f)\n",
    "        \n",
    "#         # حفظ معلومات الفهرس\n",
    "#         meta_data = [self.term_id, self.doc_id]\n",
    "#         with open(meta_path, 'wb') as f:\n",
    "#             pickle.dump(meta_data, f)\n",
    "\n",
    "# # مثال تطبيقي\n",
    "# data = {'id': [\"TheJungle\", \"FruitTypes\", \"HealthyLife\"],\n",
    "#         'ntext': [['tree', 'garden', 'juice'], ['apple', 'fruit'], ['apple', 'juice', 'fruit']]}\n",
    "\n",
    "# documents_df = pd.DataFrame(data)\n",
    "\n",
    "# # إنشاء نموذج الفهرسة\n",
    "# index_model = IndexModel(documents_df=documents_df)\n",
    "\n",
    "# # إنشاء الفهرس\n",
    "# index_model.index_new_create()\n",
    "\n",
    "# # طباعة الأرقام التعريفية للمصطلحات والوثائق\n",
    "# print(f\"Term Id: {index_model.term_id}\")\n",
    "# print(f\"Doc Id: {index_model.doc_id}\")\n",
    "\n",
    "# # طباعة مصفوفة TDIM\n",
    "# print(\"TDIM Index:\")\n",
    "# for row in index_model.tdim_index:\n",
    "#     print(row)\n",
    "\n",
    "# # الحصول على متجه وقوع المصطلح \"apple\"\n",
    "# apple_vector = index_model.vector_term_get('apple')\n",
    "# print(f\"متجه وقوع المصطلح 'apple': {apple_vector}\")\n",
    "\n",
    "# # حفظ الفهرس ومعلوماته\n",
    "# index_model.save_index('index.myindex', 'meta.ids_td')\n",
    "\n",
    "# # قراءة الفهرس ومعلوماته من الملفات\n",
    "# new_index_model = IndexModel(index_path='index.myindex', meta_path='meta.ids_td')\n",
    "\n",
    "# # طباعة الأرقام التعريفية للمصطلحات والوثائق من الفهرس المحمل\n",
    "# print(f\"Loaded Term Id: {new_index_model.term_id}\")\n",
    "# print(f\"Loaded Doc Id: {new_index_model.doc_id}\")\n",
    "\n",
    "# # طباعة مصفوفة TDIM من الفهرس المحمل\n",
    "# print(\"Loaded TDIM Index:\")\n",
    "# for row in new_index_model.tdim_index:\n",
    "#     print(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
